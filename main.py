import torch
from torch.utils.data import Dataset, DataLoader
from transformers import LongformerTokenizer, LongformerForSequenceClassification, Trainer, TrainingArguments
import pandas as pd
import psycopg2

# Establish a connection to the database and fetch data
def fetch_data():
    db_params = {
        'host': 'your_host',
        'database': 'your_database',
        'user': 'your_user',
        'password': 'your_password'
    }
    conn = psycopg2.connect(**db_params)
    query = "SELECT llm_text, family_name FROM your_table_name;"
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

# Preprocess and prepare dataset
df = fetch_data()
df['label'] = df['family_name'].astype('category').cat.codes  # Convert labels to numerical

class MalwareDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Tokenizer and Longformer model
tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-16000-model-base-4096')
max_len = 512
model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-16000-model-base-4096', num_labels=df['label'].nunique())

# Dataset and DataLoader
dataset = MalwareDataset(df['llm_text'].tolist(), df['label'].tolist(), tokenizer, max_len)
data_loader = DataLoader(dataset, batch_size=4, shuffle=True)

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

# trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

# Fine-tune the model
trainer.train()
