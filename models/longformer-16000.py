
import logging
import os
import sys

project_root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
sys.path.append(project_root)
from utils.database_utils.database_malware_dataset import StratifiedDatabaseMalwareDatasetSqlite

logging.basicConfig(level=logging.INFO)


from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, \
    EarlyStoppingCallback
from sklearn.metrics import accuracy_score

# Define a compute_metrics function
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    return {'accuracy': acc}


# Update TrainingArguments to log validation accuracy every 10 steps
training_args = TrainingArguments(
    output_dir='./bert-results',
    num_train_epochs=200,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,  # Log training info every 10 steps
    evaluation_strategy="steps",
    eval_steps=100,  # Evaluate and log accuracy on the validation set every 10 steps
    save_strategy="steps",
    save_steps=500,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    fp16=True,
    dataloader_num_workers=10
)


if __name__ == '__main__':
    # Initialize the tokenizer and model
    local_model_path = "longformer-16000-model"
    local_tokenizer_path = "longformer-16000-tokenizer"
    tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path)


    train_dataset = StratifiedDatabaseMalwareDatasetSqlite(tokenizer=tokenizer, version= 4000, split='train', max_length=1000)
    val_dataset = StratifiedDatabaseMalwareDatasetSqlite(tokenizer=tokenizer, version= 4000, split='validation', max_length=1000)

    num_labels = train_dataset.get_num_labels()
    model = AutoModelForSequenceClassification.from_pretrained(local_model_path, num_labels=num_labels)

    # Initialize trainer with EarlyStoppingCallback
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,  # Use your actual train dataset
        eval_dataset=val_dataset,  # Use your actual eval dataset
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_threshold=0.01, early_stopping_patience=3)]
        # Add early stopping callback
    )

    # Train the model
    trainer.train()
    # Save the model and tokenizer
    model.save_pretrained("./trained_model")
    tokenizer.save_pretrained("./trained_model")
