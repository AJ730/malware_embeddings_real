import os
import sys

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, \
    EarlyStoppingCallback
from sklearn.metrics import accuracy_score

project_root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
sys.path.append(project_root)
from utils.database_utils.database_malware_dataset import StratifiedDatabaseMalwareDatasetSqlite


# Define compute_metrics function for evaluation
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    return {'accuracy': acc}


if __name__ == '__main__':
    # Initialize the tokenizer
    local_model_path = "./llama-2-7b-model"
    local_tokenizer_path = "./llama-2-7b-tokenizer"
    tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path)

    # Check if the tokenizer has an EOS token
    if tokenizer.eos_token is not None:
        tokenizer.pad_token = tokenizer.eos_token
    else:
        # Add a new pad token ([PAD]) if the tokenizer doesn't have an EOS token
        # This step might require adjusting based on the tokenizer's specifics and the model's compatibility
        new_token = "[PAD]"
        tokenizer.add_special_tokens({'pad_token': new_token})
        tokenizer.pad_token = new_token
    # Verify pad_token is set
    assert tokenizer.pad_token is not None, "Tokenizer does not have a pad token set."

    train_dataset = StratifiedDatabaseMalwareDatasetSqlite(tokenizer=tokenizer, version=10000, split='train',
                                                           max_length=16384)
    val_dataset = StratifiedDatabaseMalwareDatasetSqlite(tokenizer=tokenizer, version=10000, split='validation',
                                                         max_length=16384)

    num_labels = train_dataset.get_num_labels()
    model = AutoModelForSequenceClassification.from_pretrained(local_model_path, num_labels=num_labels)
    model.resize_token_embeddings(len(tokenizer))

    # Define training arguments
    # Update TrainingArguments to log validation accuracy every 10 steps
    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=200,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=200,  # Log training info every 10 steps
        evaluation_strategy="steps",
        eval_steps=100,  # Evaluate and log accuracy on the validation set every 10 steps
        save_strategy="steps",
        save_steps=500,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        greater_is_better=True,
        dataloader_num_workers=10
    )

    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
    )

    # Train the model
    trainer.train()
