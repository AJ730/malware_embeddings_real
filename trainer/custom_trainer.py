import os
import sys


project_root = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
sys.path.append(project_root)
from utils.database_utils.database_malware_dataset import StratifiedDatabaseMalwareDatasetSqlite


import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup
from sklearn.metrics import accuracy_score

class CustomTrainer:
    def __init__(self, model, train_dataset, val_dataset, tokenizer, device, learning_rate=5e-5, epochs=100, batch_size=4,
                 save_path='./best_model', num_workers=4):
        self.model = model.to(device)
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.tokenizer = tokenizer
        self.device = device
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.batch_size = batch_size
        self.save_path = save_path
        self.best_val_accuracy = 0.0
        self.num_workers = num_workers
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate)
        self.criterion = nn.CrossEntropyLoss()

        # Total number of training steps
        self.total_steps = len(train_dataset) // batch_size * epochs
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer, num_warmup_steps=0, num_training_steps=self.total_steps)

    def train_epoch(self, dataloader):
        self.model.train()
        total_loss = 0
        correct_predictions = 0
        total_predictions = 0

        for batch in dataloader:
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            self.optimizer.zero_grad()
            outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits
            loss.backward()
            self.optimizer.step()
            self.scheduler.step()

            total_loss += loss.item()
            _, predictions = torch.max(logits, dim=1)
            correct_predictions += (predictions == labels).sum().item()
            total_predictions += labels.size(0)
            print(total_loss)

        avg_loss = total_loss / len(dataloader)
        accuracy = correct_predictions / total_predictions
        return avg_loss, accuracy

    def evaluate(self, dataloader):
        self.model.eval()
        total_loss = 0
        correct_predictions = 0
        total_predictions = 0

        for batch in dataloader:
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            with torch.no_grad():
                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss
                logits = outputs.logits

            total_loss += loss.item()
            _, predictions = torch.max(logits, dim=1)
            correct_predictions += (predictions == labels).sum().item()
            total_predictions += labels.size(0)

        avg_loss = total_loss / len(dataloader)
        accuracy = correct_predictions / total_predictions
        return avg_loss, accuracy

    def train(self):
        train_dataloader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)
        val_dataloader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)

        for epoch in range(self.epochs):
            print(f'Epoch {epoch+1}/{self.epochs}')
            train_loss, train_accuracy = self.train_epoch(train_dataloader)
            val_loss, val_accuracy = self.evaluate(val_dataloader)

            print(f'Train loss: {train_loss:.4f}, Train accuracy: {train_accuracy:.4f}')
            print(f'Validation loss: {val_loss:.4f}, Validation accuracy: {val_accuracy:.4f}')

            if val_accuracy > self.best_val_accuracy:
                print('Validation accuracy improved, saving model...')
                self.best_val_accuracy = val_accuracy
                torch.save(self.model.state_dict(), self.save_path)

    def load_best_model(self):
        self.model.load_state_dict(torch.load(self.save_path))

if __name__ == '__main__':
    local_model_path = "../models/longformer-16000-model"
    local_tokenizer_path = "../models/longformer-16000-tokenizer"
    tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path)


    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    train_dataset = StratifiedDatabaseMalwareDatasetSqlite(tokenizer=tokenizer, version=10000,  split='train', max_length=10000)
    num_labels = train_dataset.get_num_labels()
    model = AutoModelForSequenceClassification.from_pretrained(local_model_path, num_labels=num_labels)

    val_dataset = StratifiedDatabaseMalwareDatasetSqlite(tokenizer=tokenizer, version=10000, split='validation', max_length=10000)

    # Initialize and run the custom trainer
    custom_trainer = CustomTrainer(model, train_dataset, val_dataset, tokenizer, device)
    custom_trainer.train()
