import json
import numpy as np
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset
import torch

class StratifiedDatabaseMalwareDatasetSqlite(Dataset):
    def __init__(self, tokenizer, split='train', version= 4000, max_length=512, data_dir='../preprocessed_data'):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.split = split

        # Initialize LabelEncoder for encoding labels, assuming labels are stored separately if needed
        self.label_encoder = LabelEncoder()

        # Paths to preprocessed data
        self.train_data_path = f'{data_dir}/longformer-2431-train.json'
        self.val_data_path = f'{data_dir}/longformer-2431-val.json'

        # Load and split the data
        self.load_and_split_data()

    def load_and_split_data(self):
        # Load preprocessed data from JSON
        if self.split == 'train':
            with open(self.train_data_path, 'r') as f:
                self.data = json.load(f)
        else:
            with open(self.val_data_path, 'r') as f:
                self.data = json.load(f)


        # Extract SHAs and labels from the loaded data
        shas = list(self.data.keys())

        labels = [self.data[sha]['label'] for sha in shas]  # Assuming the label is stored within feature JSON

        # Fit LabelEncoder and transform labels to numerical
        encoded_labels = self.label_encoder.fit_transform(labels)

        # Update self.data to only keep SHAs for indexing in __getitem__
        self.shas = shas
        self.labels = {sha: encoded_labels[i] for i, sha in enumerate(shas)}

    def __len__(self):
        return len(self.shas)

    def __getitem__(self, idx):
        sha = self.shas[idx]
        feature_json = self.data[sha]['features']  # Assuming features are stored under 'features' key
        label = self.labels[sha]

        # Tokenize the feature dictionary
        inputs = self.tokenizer(
            json.dumps(feature_json),
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        input_ids = inputs['input_ids'].squeeze()
        attention_mask = inputs['attention_mask'].squeeze()
        label_tensor = torch.tensor(label, dtype=torch.long)

        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": label_tensor}

    def get_num_labels(self):
        return len(self.label_encoder.classes_)
