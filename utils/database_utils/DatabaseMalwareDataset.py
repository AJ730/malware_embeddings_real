import json

import numpy as np
import psycopg2
import torch
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset
from feature_extractor import generate_json_of_features
from utils.database_utils.database_extractor import db_params



class StratifiedDatabaseMalwareDataset(Dataset):
    def __init__(self, tokenizer, split='train', max_length=512, min_samples_per_class=10):
        self.tokenizer = tokenizer
        self.db_params = db_params
        self.max_length = max_length
        self.split = split

        # Initialize database connection
        self.conn = psycopg2.connect(**db_params)
        self.cur = self.conn.cursor()
        self.min_samples_per_class = min_samples_per_class
        # Initialize LabelEncoder for encoding labels
        self.label_encoder = LabelEncoder()

        # Load and split the data
        self.load_and_split_data()

    def load_and_split_data(self):
        # Fetch all SHA, family_name from the database
        self.cur.execute("SELECT sha, family_name FROM files;")
        data = self.cur.fetchall()

        # Separate SHA and family names
        shas, family_names = zip(*data)
        shas = list(shas)
        family_names = list(family_names)

        # Encode family names
        encoded_labels = self.label_encoder.fit_transform(family_names)
        unique_labels, label_counts = np.unique(encoded_labels, return_counts=True)

        # Splitting data ensuring small classes are only in training and respecting the 80-20 split
        sha_train, sha_val = [], []
        label_to_shas = {label: [] for label in unique_labels}

        # Group SHAs by their labels
        for sha, label in zip(shas, encoded_labels):
            label_to_shas[label].append(sha)

        # Split data for each class
        for label, shas_for_label in label_to_shas.items():
            # Directly add to training if less than the minimum required samples
            if label_counts[label] < self.min_samples_per_class or len(shas_for_label) < 10:
                sha_train.extend(shas_for_label)
            else:
                # Split the rest with train_test_split for a stratified split
                shas_train_split, shas_val_split = train_test_split(shas_for_label, test_size=0.2, random_state=42)
                sha_train.extend(shas_train_split)
                sha_val.extend(shas_val_split)

        # Now, depending on the split, set the dataset's data and labels
        if self.split == 'train':
            self.data = sha_train
        else:
            self.data = sha_val
        self.labels = {sha: label for sha, label in zip(shas, encoded_labels) if sha in self.data}

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sha = self.data[idx]
        label = self.labels[sha]

        # Fetch features using SHA
        feature_json = generate_json_of_features(sha)  # This function needs to be defined
        feature_dict = json.loads(feature_json)

        # Process features and tokenize
        features_as_text = ' '.join([f"{k} {v}" for k, v in feature_dict.items()])
        inputs = self.tokenizer(features_as_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors="pt")

        input_ids = inputs.input_ids.squeeze()
        attention_mask = inputs.attention_mask.squeeze()

        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": torch.tensor(label, dtype=torch.long)}

    def get_num_labels(self):
        # Return the number of unique labels
        return len(self.label_encoder.classes_)

    def close(self):
        # Close the database connection
        self.cur.close()
        self.conn.close()


class LazyDatabaseMalwareDataset(Dataset):
    def __init__(self, tokenizer,  split='train', max_length=512, split_ratio=0.8):
        self.tokenizer = tokenizer
        self.db_params = db_params
        self.max_length = max_length
        self.split = split
        self.split_ratio = split_ratio

        # Initialize database connection
        self.conn = psycopg2.connect(**db_params)
        self.cur = self.conn.cursor()

        # Calculate split sizes - this query assumes you have a sequential ID or similar
        self.cur.execute("SELECT COUNT(*) FROM files;")
        total_rows = self.cur.fetchone()[0]
        self.train_size = int(total_rows * self.split_ratio)
        self.label_encoder = self.initialize_label_encoder()

        # For simplicity in demonstration, not shuffling data
        self.length = self.train_size if split == 'train' else total_rows - self.train_size

    def initialize_label_encoder(self):
        # Fetch all unique family names to fit the LabelEncoder
        self.cur.execute("SELECT DISTINCT family_name FROM files;")
        labels = [row[0] for row in self.cur.fetchall()]
        label_encoder = LabelEncoder()
        label_encoder.fit(labels)
        return label_encoder

    def get_num_labels(self):
        # Return the number of unique labels
        return len(self.label_encoder.classes_)


    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        if self.split == 'train':
            query = f"SELECT sha, family_name FROM files ORDER BY sha LIMIT 1 OFFSET %s;"
        else:
            query = f"SELECT sha, family_name FROM files ORDER BY sha LIMIT 1 OFFSET %s;"
            idx += self.train_size  # Adjust index for eval split

        self.cur.execute(query, (idx,))
        sha, family_name = self.cur.fetchone()
        # Use the SHA to fetch features lazily
        feature_json = generate_json_of_features(sha)
        feature_dict = json.loads(feature_json)

        # Process features and tokenize
        features_as_text = ' '.join([f"{k} {v}" for k, v in feature_dict.items()])
        inputs = self.tokenizer(features_as_text, max_length=self.max_length, padding='max_length', truncation=True,
                                return_tensors="pt")

        input_ids = inputs.input_ids.squeeze()
        attention_mask = inputs.attention_mask.squeeze()
        label = torch.tensor(self.label_encoder.transform([family_name])[0], dtype=torch.long)

        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": label, "sha": sha}

    def close(self):
        # Close the database connection
        self.cur.close()
        self.conn.close()

