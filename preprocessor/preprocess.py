import concurrent.futures
import os
import sqlite3
import json
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from concurrent.futures import ProcessPoolExecutor
from tqdm import tqdm
from transformers import AutoTokenizer

from feature_extractor import  generate_json_of_features_f
from utils.database_utils.database_extractor import extract_features

db_path = '../malware_db.sqlite'
output_dir = '../preprocessed_data'
min_samples_per_class = 10

def process_row(row, truncation):
    sha, llm_text, family_name = row
    feature_dict = generate_json_of_features_f(extract_features(llm_text, truncation))

    return sha, feature_dict, family_name

def preprocess_data(truncation=10000):
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()
    cur.execute("SELECT sha, llm_text, family_name FROM files;")
    rows = cur.fetchall()

    # Temporarily store family names to encode them later
    family_names = {row[0]: row[2] for row in rows}

    processed_data = {}
    labels = {}

    with ProcessPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(process_row, row, truncation) for row in rows]
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(rows), desc="Processing rows"):
            sha, feature_json, family_name = future.result()
            processed_data[sha] = feature_json
            labels[sha] = family_name

    # Encode labels
    label_encoder = LabelEncoder()
    encoded_labels = label_encoder.fit_transform(list(family_names.values()))
    # Map SHA to its encoded label
    encoded_labels_dict = {sha: encoded_labels[i] for i, sha in enumerate(family_names.keys())}

    sha_train, sha_val = stratified_split(list(processed_data.keys()), list(encoded_labels_dict.values()))

    save_preprocessed_data(sha_train, sha_val, processed_data, encoded_labels_dict, output_dir)

    cur.close()
    conn.close()


def stratified_split(shas, encoded_labels):
    sha_train, sha_val = [], []
    label_to_shas = {}

    for sha, label in zip(shas, encoded_labels):
        if label not in label_to_shas:
            label_to_shas[label] = []
        label_to_shas[label].append(sha)



    for label, shas_for_label in label_to_shas.items():
        if len(shas_for_label) < min_samples_per_class:
            sha_train.extend(shas_for_label)
        else:
            shas_train_split, shas_val_split = train_test_split(shas_for_label, test_size=0.2, random_state=42)
            sha_train.extend(shas_train_split)
            sha_val.extend(shas_val_split)

    return sha_train, sha_val

def save_preprocessed_data(sha_train, sha_val, processed_data, encoded_labels, output_dir):
    # Convert NumPy int64 to Python int for JSON serialization
    train_data = {sha: {'features': processed_data[sha], 'label': int(encoded_labels[sha])} for sha in sha_train}
    val_data = {sha: {'features': processed_data[sha], 'label': int(encoded_labels[sha])} for sha in sha_val}

    # Ensure directory exists
    os.makedirs(output_dir, exist_ok=True)

    with open(f'{output_dir}/longformer-test-train.json', 'w') as f:
        json.dump(train_data, f)
    with open(f'{output_dir}/longformer-test-val.json', 'w') as f:
        json.dump(val_data, f)


if __name__ == '__main__':
    # # # # Path to the train_data.json file
    json_file_path = '../preprocessed_data/longformer-test-train.json'
    local_model_path = "../models/longformer-4096-model"
    local_tokenizer_path = "../models/longformer-4096-tokenizer"
    tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path)

    # Load the JSON data
    with open(json_file_path, 'r') as file:
        data = json.load(file)

    # Get a single SHA key from the dictionary


    # Sample data from data[single_sha]
    sample_data = data['c6ba51eaebe94c305d4ab38c66371a0d1aaaf6679248fcf592b2046cc4bc6398']

    # Serialize the data to a JSON string
    json_data = json.dumps(sample_data)

    # Tokenize the JSON string
    tokens = tokenizer.tokenize(json_data)

    # Count the number of tokens
    num_tokens = len(tokens)

    print(num_tokens)

    # print(f"Number of tokens in data[single_sha]: {num_tokens}")
    preprocess_data(truncation=100)

